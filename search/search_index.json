{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction What is Plume? Plume is a JVM bytecode to code property graph library supported by a graph database storage backend. Given an application compiled to JVM bytecode, the library will analyze the bytecode using an interprocedural control-flow graph (ICFG) produced using Soot . This ICFG is then converted into what is called a code property graph which, at the base level, is simply a combination of an abstract syntax tree (AST), control flow graph (CFG), and program dependence graph (PDG). This three part graph is then persisted in a supported graph database and can be queried and analyzed via tools such as Joern . Note that Plume produces CPGs ideally used for dataflow tracking. The AST's from using Jimple as the IR lead to fairly degenerated ASTs which make Plume unsuitable for AST analysis. Plume can be downloaded via . Optimized in Joern as jimple2cpg Plume is the original implementation of jimple2cpg . The frontend on the Joern project is optimized around OverflowDB and is much more lightweight. This is project focuses on experimenting with incremental dataflow analysis and comparing database backend performance. Benefits of using Plume Plume is an open-source Scala project which provides a type-safe interface for interacting with a graph database constructed using ShiftLeft's (the primary maintainers of Fabian Yamaguchi's Joern ) CPG schema . The idea of storing the CPG in a graph database is motivated by the observation that this approach will allow the analysis to be done incrementally (one does not have to regenerate a graph everytime one wishes to perform analysis and results can be persisted), updates done partially (if one method changes, only that subtree is regenerated), and be scalable for large applications due to how the number of nodes and edges scale for the CPG. Plume supports multiple graph databases so that developers can select a graph database based on their software stack and processing requirements. Plume provides JVM bytecode support for the Joern project which means that one can use Joern to perform analysis on Plume generated CPGs. Supported Languages Since Plume analyzes JVM bytecode, if a language is able to compile to JVM bytecode then Plume can accept it. Plume no longer supports compiling Java source code to bytecode automatically. Simply load the respective .jar or .class files (or a directory or JAR file containing either) using the Jimple2Cpg::createCpg method accordingly. General Plume Benefits Choice of graph database: in-memory to dedicated, open-source to enterprise, single node to multi-machine clustered. CPG schema strictly enforced in the Plume driver using codepropertygraph domain classes. Handles very large code property graphs. Analysis can be done incrementally. Simple to use interface. Open source under the liberal Apache 2 license . Sponsored by Where does the name come from? The word \"plume\" can describe a plume of smoke, dust, or fire rising into the air in a column in large quantities. Due to the fact that Plume leverages Soot to construct the code property graph, a colleague of mine, Lauren Hayward , suggested it be called Plume as homage and so it was named.","title":"Introduction"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#what-is-plume","text":"Plume is a JVM bytecode to code property graph library supported by a graph database storage backend. Given an application compiled to JVM bytecode, the library will analyze the bytecode using an interprocedural control-flow graph (ICFG) produced using Soot . This ICFG is then converted into what is called a code property graph which, at the base level, is simply a combination of an abstract syntax tree (AST), control flow graph (CFG), and program dependence graph (PDG). This three part graph is then persisted in a supported graph database and can be queried and analyzed via tools such as Joern . Note that Plume produces CPGs ideally used for dataflow tracking. The AST's from using Jimple as the IR lead to fairly degenerated ASTs which make Plume unsuitable for AST analysis. Plume can be downloaded via . Optimized in Joern as jimple2cpg Plume is the original implementation of jimple2cpg . The frontend on the Joern project is optimized around OverflowDB and is much more lightweight. This is project focuses on experimenting with incremental dataflow analysis and comparing database backend performance.","title":"What is Plume?"},{"location":"#benefits-of-using-plume","text":"Plume is an open-source Scala project which provides a type-safe interface for interacting with a graph database constructed using ShiftLeft's (the primary maintainers of Fabian Yamaguchi's Joern ) CPG schema . The idea of storing the CPG in a graph database is motivated by the observation that this approach will allow the analysis to be done incrementally (one does not have to regenerate a graph everytime one wishes to perform analysis and results can be persisted), updates done partially (if one method changes, only that subtree is regenerated), and be scalable for large applications due to how the number of nodes and edges scale for the CPG. Plume supports multiple graph databases so that developers can select a graph database based on their software stack and processing requirements. Plume provides JVM bytecode support for the Joern project which means that one can use Joern to perform analysis on Plume generated CPGs.","title":"Benefits of using Plume"},{"location":"#supported-languages","text":"Since Plume analyzes JVM bytecode, if a language is able to compile to JVM bytecode then Plume can accept it. Plume no longer supports compiling Java source code to bytecode automatically. Simply load the respective .jar or .class files (or a directory or JAR file containing either) using the Jimple2Cpg::createCpg method accordingly.","title":"Supported Languages"},{"location":"#general-plume-benefits","text":"Choice of graph database: in-memory to dedicated, open-source to enterprise, single node to multi-machine clustered. CPG schema strictly enforced in the Plume driver using codepropertygraph domain classes. Handles very large code property graphs. Analysis can be done incrementally. Simple to use interface. Open source under the liberal Apache 2 license .","title":"General Plume Benefits"},{"location":"#sponsored-by","text":"","title":"Sponsored by"},{"location":"#where-does-the-name-come-from","text":"The word \"plume\" can describe a plume of smoke, dust, or fire rising into the air in a column in large quantities. Due to the fact that Plume leverages Soot to construct the code property graph, a colleague of mine, Lauren Hayward , suggested it be called Plume as homage and so it was named.","title":"Where does the name come from?"},{"location":"scala-doc/","text":"The documentation for the API is written using ScalaDoc. These docs can be found here .","title":"ScalaDoc API"},{"location":"getting-started/architectural-overview/","text":"Architectural Overview Plume is divided into two major parts: an extractor and a driver. Each part can then be subdivided further but this separation of concerns allows for each part of the CPG analysis lifecycle to be distributed between different parts of a program. Driver The driver provides the interface via which one can communicate to a chosen graph database. This interface is defined by IDriver and is what is implemented by the database drivers such as the TinkerGraph driver or TigerGraph driver . The extractor and analyser make use of the driver but one can use the driver independently to perform one's own CPG construction or analysis. Currently there are only a handful of graph databases supported by the driver but we are continually looking at supporting more graph databases. Extractor The extractor is in charge of constructing the code property graph from bytecode. The entrypoint for this component is the Jimple2Cpg class where one can load and project the CPG using the given driver and classpath. Either class or JAR files can be loaded. The extractor makes use of Soot to convert the class files into Jimple from which the call graph is constructed. Soot's SootMethod is used in order to construct the method bodies from AST information. AST ordering and control targets are then used to derive a CFG from which the rest of the dependencies and subgraphs are derived from. Due to the fact that Plume constructs the CPG from bytecode, the graph produced is not completely interchangeable with the source code (and even less after the transformation to Jimple!) but line number and dataflow information is accurately preserved.","title":"Architectural Overview"},{"location":"getting-started/architectural-overview/#architectural-overview","text":"Plume is divided into two major parts: an extractor and a driver. Each part can then be subdivided further but this separation of concerns allows for each part of the CPG analysis lifecycle to be distributed between different parts of a program.","title":"Architectural Overview"},{"location":"getting-started/architectural-overview/#driver","text":"The driver provides the interface via which one can communicate to a chosen graph database. This interface is defined by IDriver and is what is implemented by the database drivers such as the TinkerGraph driver or TigerGraph driver . The extractor and analyser make use of the driver but one can use the driver independently to perform one's own CPG construction or analysis. Currently there are only a handful of graph databases supported by the driver but we are continually looking at supporting more graph databases.","title":"Driver"},{"location":"getting-started/architectural-overview/#extractor","text":"The extractor is in charge of constructing the code property graph from bytecode. The entrypoint for this component is the Jimple2Cpg class where one can load and project the CPG using the given driver and classpath. Either class or JAR files can be loaded. The extractor makes use of Soot to convert the class files into Jimple from which the call graph is constructed. Soot's SootMethod is used in order to construct the method bodies from AST information. AST ordering and control targets are then used to derive a CFG from which the rest of the dependencies and subgraphs are derived from. Due to the fact that Plume constructs the CPG from bytecode, the graph produced is not completely interchangeable with the source code (and even less after the transformation to Jimple!) but line number and dataflow information is accurately preserved.","title":"Extractor"},{"location":"getting-started/basic-usage/","text":"Basic Usage Introduction The cycle of using Plume for static analysis starts with loading the target program via the extractor. This assumes one has already decided on what storage backend one wishes to use. All source code for the example below with all supported storage backends can be found on the examples repository and the following tutorial is a simplified version of the TinkerGraphApp example. Plume is coded to be as platform independent as possible but the examples below are simplified to work only on Unix based systems for readability e.g. the use of forward-slashes when referencing the file system. If using Windows, replace all forward-slashes with back-slashes in the examples below or make use of File.separator constant. Setup and Configuration Plume can be pulled from JitPack as follows: Remember to change X.X.X to libraryDependencies += com.github.plume-oss % plume % X.X.X The Plume libraries can be obtained by running the following convenience script which obtains the bleeding edge version: #!/bin/bash # Download latest stable Plume libraries rm -rf lib && mkdir -p lib && rm -rf ./tmp && mkdir -p ./tmp && cd ./tmp \\ && git init \\ && git remote add origin https://github.com/plume-oss/plume.git \\ && git fetch --depth 1 origin develop \\ && git reset --hard FETCH_HEAD \\ && sbt stage \\ && mv ./target/scala-2.13/plume_2.13-X.X.X.jar ../lib/plume_2.13-X.X.X.jar \\ && cd .. && rm -rf ./tmp Creating a driver Before we can extract our graph let's set up a simple in-memory driver. The driver will create a \"connection\" (i.e. create an instance of the backend) on creation of the driver object. e.g. import com . github . plume . oss . drivers . OverflowDbDriver object OverflowDbApp { def main ( args : Array [ String ]): Unit = { val dbOutputFile = \"cpg.odb\" new OverflowDbDriver ( storageLocation = Some ( dbOutputFile ) ) } } Extracting a CPG Once we have a driver, we can extract a CPG from bytecode. Our extractor takes the desired driver and the root directory (as a String path) of the classes we wish to analyse as arguments. Let's assume the classes we want to analyze are next to our TinkerGraphApp in a folder called example . import com . github . plume . oss . Jimple2Cpg import com . github . plume . oss . drivers . TinkerGraphDriver import scala . util . Using object TinkerGraphApp { def main ( args : Array [ String ]): Unit = { val targetDir = \"./example\" val dbOutputFile = \"cpg.xml\" println ( \"Creating driver\" ) Using . resource ( new TinkerGraphDriver ()) { d => println ( s\"Creating CPG from .class files found under $ targetDir \" ) new Jimple2Cpg (). createCpg ( rawSourceCodePath = targetDir , driver = d ) d . exportGraph ( dbOutputFile ) } println ( s\"Done! CPG persisted at $ dbOutputFile \" ) } } Loading and projecting files Next we would like to load the files we would like to extract the code property graph from. Classes can be loaded as class files, JAR files, or a directory containing either. For this example we will load the following file and store it under the package structure intraprocedural/basic/Basic1.java so the full path will be ./example/intraprocedural/basic/Basic1.java . package intraprocedural.basic ; public class Basic1 { public static void main ( String [] args ) { int a = 3 ; int b = 2 ; int c = a + b ; } } We can then compile the source code using javac -g Basic1.java . The -g argument adds debugging information which helps map our results back to the source code. The rawSourceCodePath argument can either be a file or directory. Now let's load the file and project it to the graph database. val exampleFile = \"./example/intraprocedural/basic/Basic1.class\" new Jimple2Cpg (). createCpg ( rawSourceCodePath = exampleFile , driver = d ) Exporting and visualizing the graph The code property graph is now constructed, in order to retrieve this from our in-memory database let's export our graph. The TinkerGraphDriver supports importing and exporting graphs in GraphML, GraphSON, and Gryo formats specified by the file extension .xml , .json , and .kryo . In this example we will export the graph in GraphML format so that it can be visualized using Cytoscape . driver . exportGraph ( \"./graph.xml\" ); With a bit of custom styling, filtering, and using the labelV property as the displayed label for vertices and edges our graph will look something like this (albeit it will be much more complex): Alternatively, if one imports an OverflowDB binary into Joern then a dot file of the graph can be generated.","title":"Basic Usage"},{"location":"getting-started/basic-usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"getting-started/basic-usage/#introduction","text":"The cycle of using Plume for static analysis starts with loading the target program via the extractor. This assumes one has already decided on what storage backend one wishes to use. All source code for the example below with all supported storage backends can be found on the examples repository and the following tutorial is a simplified version of the TinkerGraphApp example. Plume is coded to be as platform independent as possible but the examples below are simplified to work only on Unix based systems for readability e.g. the use of forward-slashes when referencing the file system. If using Windows, replace all forward-slashes with back-slashes in the examples below or make use of File.separator constant.","title":"Introduction"},{"location":"getting-started/basic-usage/#setup-and-configuration","text":"Plume can be pulled from JitPack as follows: Remember to change X.X.X to libraryDependencies += com.github.plume-oss % plume % X.X.X The Plume libraries can be obtained by running the following convenience script which obtains the bleeding edge version: #!/bin/bash # Download latest stable Plume libraries rm -rf lib && mkdir -p lib && rm -rf ./tmp && mkdir -p ./tmp && cd ./tmp \\ && git init \\ && git remote add origin https://github.com/plume-oss/plume.git \\ && git fetch --depth 1 origin develop \\ && git reset --hard FETCH_HEAD \\ && sbt stage \\ && mv ./target/scala-2.13/plume_2.13-X.X.X.jar ../lib/plume_2.13-X.X.X.jar \\ && cd .. && rm -rf ./tmp","title":"Setup and Configuration"},{"location":"getting-started/basic-usage/#creating-a-driver","text":"Before we can extract our graph let's set up a simple in-memory driver. The driver will create a \"connection\" (i.e. create an instance of the backend) on creation of the driver object. e.g. import com . github . plume . oss . drivers . OverflowDbDriver object OverflowDbApp { def main ( args : Array [ String ]): Unit = { val dbOutputFile = \"cpg.odb\" new OverflowDbDriver ( storageLocation = Some ( dbOutputFile ) ) } }","title":"Creating a driver"},{"location":"getting-started/basic-usage/#extracting-a-cpg","text":"Once we have a driver, we can extract a CPG from bytecode. Our extractor takes the desired driver and the root directory (as a String path) of the classes we wish to analyse as arguments. Let's assume the classes we want to analyze are next to our TinkerGraphApp in a folder called example . import com . github . plume . oss . Jimple2Cpg import com . github . plume . oss . drivers . TinkerGraphDriver import scala . util . Using object TinkerGraphApp { def main ( args : Array [ String ]): Unit = { val targetDir = \"./example\" val dbOutputFile = \"cpg.xml\" println ( \"Creating driver\" ) Using . resource ( new TinkerGraphDriver ()) { d => println ( s\"Creating CPG from .class files found under $ targetDir \" ) new Jimple2Cpg (). createCpg ( rawSourceCodePath = targetDir , driver = d ) d . exportGraph ( dbOutputFile ) } println ( s\"Done! CPG persisted at $ dbOutputFile \" ) } }","title":"Extracting a CPG"},{"location":"getting-started/basic-usage/#loading-and-projecting-files","text":"Next we would like to load the files we would like to extract the code property graph from. Classes can be loaded as class files, JAR files, or a directory containing either. For this example we will load the following file and store it under the package structure intraprocedural/basic/Basic1.java so the full path will be ./example/intraprocedural/basic/Basic1.java . package intraprocedural.basic ; public class Basic1 { public static void main ( String [] args ) { int a = 3 ; int b = 2 ; int c = a + b ; } } We can then compile the source code using javac -g Basic1.java . The -g argument adds debugging information which helps map our results back to the source code. The rawSourceCodePath argument can either be a file or directory. Now let's load the file and project it to the graph database. val exampleFile = \"./example/intraprocedural/basic/Basic1.class\" new Jimple2Cpg (). createCpg ( rawSourceCodePath = exampleFile , driver = d )","title":"Loading and projecting files"},{"location":"getting-started/basic-usage/#exporting-and-visualizing-the-graph","text":"The code property graph is now constructed, in order to retrieve this from our in-memory database let's export our graph. The TinkerGraphDriver supports importing and exporting graphs in GraphML, GraphSON, and Gryo formats specified by the file extension .xml , .json , and .kryo . In this example we will export the graph in GraphML format so that it can be visualized using Cytoscape . driver . exportGraph ( \"./graph.xml\" ); With a bit of custom styling, filtering, and using the labelV property as the displayed label for vertices and edges our graph will look something like this (albeit it will be much more complex): Alternatively, if one imports an OverflowDB binary into Joern then a dot file of the graph can be generated.","title":"Exporting and visualizing the graph"},{"location":"plume-basics/code-property-graph/","text":"The Code Property Graph The inventor of the code property graph and Chief Scientist at ShiftLeft, Fabian Yamaguchi, explains that the code property graph is a concept based on a simple observation: there are many different graph representations of code, and patterns in code can often be expressed as patterns in these graphs. This schema is now primarily maintained by ShiftLeft and can be viewed on GitHub . Early implementations of this schema were seen on Neo4j and ShiftLeft's fork of TinkerGraph which has now diverged significantly and known as OverflowDB . Plume makes use of the concept of the code property graph and this is enforced by interacting with the storage layer using the driver. Understanding the Code Property Graph The code property graph merges three graph representations of code; namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. The following illustration is taken from the paper \"Modeling and Discovering Vulnerabilities with Code Property Graphs\" . Since the property graph model is native to many popular graph database storage models the CPG can be stored and queried within graph databases. The CPG explodes in size as programs get larger so having scalability and resource management handled by a graph database simplifies this problem. Incremental Analysis By persisting the graph in the database, one can also store results from the analysis on the stored graph too. Examples of such analysis could be, by starting with a raw CPG, adding the dominator or data-flow edges to the graph or performing constant propagation. This allows for code optimization and the re-use of results. Some analysis could further differentiate the program when compared to the source code but many graph databases support multiple graphs and one could store snapshots before each analysis. As one updates a part of the original code the whole graph does not need to be re-generated only the affected subgraph. This allows for successive analysis to be much faster and updates to be minor.","title":"Code Property Graph"},{"location":"plume-basics/code-property-graph/#the-code-property-graph","text":"The inventor of the code property graph and Chief Scientist at ShiftLeft, Fabian Yamaguchi, explains that the code property graph is a concept based on a simple observation: there are many different graph representations of code, and patterns in code can often be expressed as patterns in these graphs. This schema is now primarily maintained by ShiftLeft and can be viewed on GitHub . Early implementations of this schema were seen on Neo4j and ShiftLeft's fork of TinkerGraph which has now diverged significantly and known as OverflowDB . Plume makes use of the concept of the code property graph and this is enforced by interacting with the storage layer using the driver.","title":"The Code Property Graph"},{"location":"plume-basics/code-property-graph/#understanding-the-code-property-graph","text":"The code property graph merges three graph representations of code; namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. The following illustration is taken from the paper \"Modeling and Discovering Vulnerabilities with Code Property Graphs\" . Since the property graph model is native to many popular graph database storage models the CPG can be stored and queried within graph databases. The CPG explodes in size as programs get larger so having scalability and resource management handled by a graph database simplifies this problem.","title":"Understanding the Code Property Graph"},{"location":"plume-basics/code-property-graph/#incremental-analysis","text":"By persisting the graph in the database, one can also store results from the analysis on the stored graph too. Examples of such analysis could be, by starting with a raw CPG, adding the dominator or data-flow edges to the graph or performing constant propagation. This allows for code optimization and the re-use of results. Some analysis could further differentiate the program when compared to the source code but many graph databases support multiple graphs and one could store snapshots before each analysis. As one updates a part of the original code the whole graph does not need to be re-generated only the affected subgraph. This allows for successive analysis to be much faster and updates to be minor.","title":"Incremental Analysis"},{"location":"plume-basics/incremental/","text":"Incremental Updates Part of why Plume stores the CPG in a backend database is to allow for long term storage of a large initial projection and only updating the parts of the graph associated with code updates going forward. Intermediate Representation Whenever code is changed, simply re-add the artifact via the Jimple2Cpg::createCpg function. There are hashes stored in the graph database to detect changes on a per class level. Due to this, the whole artifact needs to be loaded each time e.g. re-build and re-load a JAR. Any missing classes will be detected as removed from the application and trimmed from the underlying storage backend. Roughly, the algorithm reacts to changes in the following order: Load each class into Plume, calculating the hash of each. Remove any missing classes from the database. These are classes in the database but not loaded into Plume. This removes that class' type, program, member, and method information. Classes with hashes differing to the files in the database are marked for update. Classes with matching hashes to the files in the database are removed from the queue. Remaining classes which need to be added or re-added are then generated. The hash used is XXH32 from xxHash which is a fast and non-cryptographic hash. Data-Flow Representation When a call to nodesReachableBy is called, data-flow paths are calculated on the fly and used to determine whether a node is reachable via a control flow path or not. These data-flow paths are then stored in a cache for future queries. During a separate session, these paths are then loaded back into the data-flow engine cache. Future Feature Loading data-flow paths is a feature scheduled for v1.1.0","title":"Incremental Updates"},{"location":"plume-basics/incremental/#incremental-updates","text":"Part of why Plume stores the CPG in a backend database is to allow for long term storage of a large initial projection and only updating the parts of the graph associated with code updates going forward.","title":"Incremental Updates"},{"location":"plume-basics/incremental/#intermediate-representation","text":"Whenever code is changed, simply re-add the artifact via the Jimple2Cpg::createCpg function. There are hashes stored in the graph database to detect changes on a per class level. Due to this, the whole artifact needs to be loaded each time e.g. re-build and re-load a JAR. Any missing classes will be detected as removed from the application and trimmed from the underlying storage backend. Roughly, the algorithm reacts to changes in the following order: Load each class into Plume, calculating the hash of each. Remove any missing classes from the database. These are classes in the database but not loaded into Plume. This removes that class' type, program, member, and method information. Classes with hashes differing to the files in the database are marked for update. Classes with matching hashes to the files in the database are removed from the queue. Remaining classes which need to be added or re-added are then generated. The hash used is XXH32 from xxHash which is a fast and non-cryptographic hash.","title":"Intermediate Representation"},{"location":"plume-basics/incremental/#data-flow-representation","text":"When a call to nodesReachableBy is called, data-flow paths are calculated on the fly and used to determine whether a node is reachable via a control flow path or not. These data-flow paths are then stored in a cache for future queries. During a separate session, these paths are then loaded back into the data-flow engine cache. Future Feature Loading data-flow paths is a feature scheduled for v1.1.0","title":"Data-Flow Representation"},{"location":"plume-basics/taint-analysis/","text":"Taint Analysis One can do basic taint analysis based on simple reachability when using the OverflowDB driver. This wraps around Joern's dataflow engine and adds an incremental aspect by saving the IFDS-style paths calculated during the query. More on this can be found in the next section. Example Let's refer to the example found on Plume Examples . Let's look at two class files where one calls the other: public class Foo { public static void main ( String [] args ) { var a = 1 ; var b = 2 ; var c = Bar . add ( a , b ); System . out . println ( c ); } } public class Bar { public static int add ( int x , int y ) { return x + y ; } } Using OverflowDbDriver::nodesReachableBy one can find which sources will reach a given sink. E.g. def taintAnalysis ( d : OverflowDbDriver ): Unit = { import io . shiftleft . semanticcpg . language . _ import io . shiftleft . codepropertygraph .{ Cpg => CPG } println ( \"Finding data flows from source identifiers with the name \" + \"'a' sinking at methods with names 'add'\" ) val cpg = CPG ( d . cpg . graph ) d . nodesReachableBy ( cpg . identifier ( \"a\" ), cpg . call ( \"add\" )) . map { n => s\" ${ n . label } : ${ n . property ( \"CODE\" ) } @ line ${ n . property ( \"LINE_NUMBER\" ) } \" } . foreach ( println ( _ )) } The code above will return something like: Finding data flows from source identifiers with the name 'a' sinking at methods with names 'add' IDENTIFIER: a @ line 4 IDENTIFIER: a @ line 6 Note that the signature is: nodesReachableBy(<source>, <sink>) . For more on how to use the nodesReachableBy call refer to the Joern Docs .","title":"Taint Analysis"},{"location":"plume-basics/taint-analysis/#taint-analysis","text":"One can do basic taint analysis based on simple reachability when using the OverflowDB driver. This wraps around Joern's dataflow engine and adds an incremental aspect by saving the IFDS-style paths calculated during the query. More on this can be found in the next section.","title":"Taint Analysis"},{"location":"plume-basics/taint-analysis/#example","text":"Let's refer to the example found on Plume Examples . Let's look at two class files where one calls the other: public class Foo { public static void main ( String [] args ) { var a = 1 ; var b = 2 ; var c = Bar . add ( a , b ); System . out . println ( c ); } } public class Bar { public static int add ( int x , int y ) { return x + y ; } } Using OverflowDbDriver::nodesReachableBy one can find which sources will reach a given sink. E.g. def taintAnalysis ( d : OverflowDbDriver ): Unit = { import io . shiftleft . semanticcpg . language . _ import io . shiftleft . codepropertygraph .{ Cpg => CPG } println ( \"Finding data flows from source identifiers with the name \" + \"'a' sinking at methods with names 'add'\" ) val cpg = CPG ( d . cpg . graph ) d . nodesReachableBy ( cpg . identifier ( \"a\" ), cpg . call ( \"add\" )) . map { n => s\" ${ n . label } : ${ n . property ( \"CODE\" ) } @ line ${ n . property ( \"LINE_NUMBER\" ) } \" } . foreach ( println ( _ )) } The code above will return something like: Finding data flows from source identifiers with the name 'a' sinking at methods with names 'add' IDENTIFIER: a @ line 4 IDENTIFIER: a @ line 6 Note that the signature is: nodesReachableBy(<source>, <sink>) . For more on how to use the nodesReachableBy call refer to the Joern Docs .","title":"Example"},{"location":"storage-backends/introduction/","text":"Introduction Plume's data storage layer is pluggable. Each graph database is called a storage backend and allows one to be flexible in which solution one requires. This section describes the use case and configuration of each storage backend with examples of how to get started. A driver for a particular storage backend can be obtained by calling the correct constructor from classes contained in the com.github.plume.oss.drivers package. import com . github . plume . oss . drivers . TigerGraphDriver val driver = new TigerGraphDriver ( hostname = \"localhost\" , restPpPort = 9000 , gsqlPort = 14240 , username = \"tigergraph\" , password = \"tigergraph\" , ) All Plume drivers implement the AutoCloseable interface and can thus be used within a scala.util.Using block.","title":"Introduction"},{"location":"storage-backends/introduction/#introduction","text":"Plume's data storage layer is pluggable. Each graph database is called a storage backend and allows one to be flexible in which solution one requires. This section describes the use case and configuration of each storage backend with examples of how to get started. A driver for a particular storage backend can be obtained by calling the correct constructor from classes contained in the com.github.plume.oss.drivers package. import com . github . plume . oss . drivers . TigerGraphDriver val driver = new TigerGraphDriver ( hostname = \"localhost\" , restPpPort = 9000 , gsqlPort = 14240 , username = \"tigergraph\" , password = \"tigergraph\" , ) All Plume drivers implement the AutoCloseable interface and can thus be used within a scala.util.Using block.","title":"Introduction"},{"location":"storage-backends/janusgraph/","text":"JanusGraph JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. \u2014 JanusGraph Documentation Introduction JanusGraph is an open-source JVM-based graph database that is queried via the Gremlin query language. JanusGraph is a split into three parts: itself (the graph database abstraction), a pluggable storage backend and a pluggable search engine. Plume communicates with JanusGraph over the Gremlin remote connection which is described in the documentation here . JanusGraph No Longer Supported As from version 1.0.0 JanusGraph is no longer a supported backend. These docs follow from older versions (<= 0.6.3) from the Kotlin-based Plume libraries. Driver Configuration and Usage JanusGraph's driver can be created as follows: val driver = ( DriverFactory ( GraphDatabase . JANUS_GRAPH ) as JanusGraphDriver ) . apply { remoteConfig ( \"src/main/resources/conf/remote-graph.properties\" ). connect () } Where one needs two configuration files remote-graph.properties and conf/remote-objects.yaml . In the example above, these two files are in src/main/resources/conf/ . For a simple local JanusGraph connection, one can use the following configurations: conf/remote-graph.properties gremlin.remote.remoteConnectionClass=org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteConnection gremlin.remote.driver.clusterFile=src/main/resources/conf/remote-objects.yaml gremlin.remote.driver.sourceName=g conf/remote-objects.yaml hosts : [ localhost ] port : 8182 serializer : { className : org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0 , config : { ioRegistries : [ org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry ] }} The driver will automatically detect if the JanusGraph connection makes use of transactions or not and will create and commit them accordingly. More information on making use of JanusGraph can be found on their documentation . Ideal Use Case JanusGraph is ideal in the case where budget for an enterprise license is limited but a dedicated, distributed, and remote graph database is required. Scylla is the recommended storage backend for a fast and efficient distributed system with ElasticSearch as the search engine. For resource sensitive single-node environments I would suggest using BerkeleyDB and Lucene . Benefits The following exerpt is taken from the documentation: Support for very large graphs. JanusGraph graphs scale with the number of machines in the cluster. Support for very many concurrent transactions and operational graph processing. JanusGraph\u2019s transactional capacity scales with the number of machines in the cluster and answers complex traversal queries on huge graphs in milliseconds. Support for global graph analytics and batch graph processing through the Hadoop framework. Native support for the popular property graph data model exposed by Apache TinkerPop . Native support for the graph traversal language Gremlin . Numerous graph-level configurations provide knobs for tuning performance. Vertex-centric indices provide vertex-level querying to alleviate issues with the infamous super node problem . Provides an optimized disk representation to allow for efficient use of storage and speed of access. Open source under the liberal Apache 2 license . Limitations JanusGraph with plugged in storage backends and search engines can become very resource intensive. An example is with Cassandra and ElasticSearch. JanusGraph is the slowest storage backend of the bunch so for large programs, indexing and enforcing the CPG schema in the configuration will become necessary. An example of this is in the Plume Examples repository. JanusGraph can store up to a quintillion edges (2^60) and half as many vertices. That limitation is imposed by JanusGraph\u2019s id scheme. Retrieving an edge by id, e.g tx.getEdge(edge.getId()) , is not a constant time operation because it requires an index call on one of its adjacent vertices. Hence, the cost of retrieving an individual edge by its id is O(log(k)) where k is the number of incident edges on the adjacent vertex. JanusGraph will attempt to pick the adjacent vertex with the smaller degree. This also applies to index retrievals for edges via a standard or external index.","title":"JanusGraph"},{"location":"storage-backends/janusgraph/#janusgraph","text":"JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. \u2014 JanusGraph Documentation Introduction JanusGraph is an open-source JVM-based graph database that is queried via the Gremlin query language. JanusGraph is a split into three parts: itself (the graph database abstraction), a pluggable storage backend and a pluggable search engine. Plume communicates with JanusGraph over the Gremlin remote connection which is described in the documentation here . JanusGraph No Longer Supported As from version 1.0.0 JanusGraph is no longer a supported backend. These docs follow from older versions (<= 0.6.3) from the Kotlin-based Plume libraries.","title":"JanusGraph"},{"location":"storage-backends/janusgraph/#driver-configuration-and-usage","text":"JanusGraph's driver can be created as follows: val driver = ( DriverFactory ( GraphDatabase . JANUS_GRAPH ) as JanusGraphDriver ) . apply { remoteConfig ( \"src/main/resources/conf/remote-graph.properties\" ). connect () } Where one needs two configuration files remote-graph.properties and conf/remote-objects.yaml . In the example above, these two files are in src/main/resources/conf/ . For a simple local JanusGraph connection, one can use the following configurations: conf/remote-graph.properties gremlin.remote.remoteConnectionClass=org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteConnection gremlin.remote.driver.clusterFile=src/main/resources/conf/remote-objects.yaml gremlin.remote.driver.sourceName=g conf/remote-objects.yaml hosts : [ localhost ] port : 8182 serializer : { className : org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0 , config : { ioRegistries : [ org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry ] }} The driver will automatically detect if the JanusGraph connection makes use of transactions or not and will create and commit them accordingly. More information on making use of JanusGraph can be found on their documentation .","title":"Driver Configuration and Usage"},{"location":"storage-backends/janusgraph/#ideal-use-case","text":"JanusGraph is ideal in the case where budget for an enterprise license is limited but a dedicated, distributed, and remote graph database is required. Scylla is the recommended storage backend for a fast and efficient distributed system with ElasticSearch as the search engine. For resource sensitive single-node environments I would suggest using BerkeleyDB and Lucene .","title":"Ideal Use Case"},{"location":"storage-backends/janusgraph/#benefits","text":"The following exerpt is taken from the documentation: Support for very large graphs. JanusGraph graphs scale with the number of machines in the cluster. Support for very many concurrent transactions and operational graph processing. JanusGraph\u2019s transactional capacity scales with the number of machines in the cluster and answers complex traversal queries on huge graphs in milliseconds. Support for global graph analytics and batch graph processing through the Hadoop framework. Native support for the popular property graph data model exposed by Apache TinkerPop . Native support for the graph traversal language Gremlin . Numerous graph-level configurations provide knobs for tuning performance. Vertex-centric indices provide vertex-level querying to alleviate issues with the infamous super node problem . Provides an optimized disk representation to allow for efficient use of storage and speed of access. Open source under the liberal Apache 2 license .","title":"Benefits"},{"location":"storage-backends/janusgraph/#limitations","text":"JanusGraph with plugged in storage backends and search engines can become very resource intensive. An example is with Cassandra and ElasticSearch. JanusGraph is the slowest storage backend of the bunch so for large programs, indexing and enforcing the CPG schema in the configuration will become necessary. An example of this is in the Plume Examples repository. JanusGraph can store up to a quintillion edges (2^60) and half as many vertices. That limitation is imposed by JanusGraph\u2019s id scheme. Retrieving an edge by id, e.g tx.getEdge(edge.getId()) , is not a constant time operation because it requires an index call on one of its adjacent vertices. Hence, the cost of retrieving an individual edge by its id is O(log(k)) where k is the number of incident edges on the adjacent vertex. JanusGraph will attempt to pick the adjacent vertex with the smaller degree. This also applies to index retrievals for edges via a standard or external index.","title":"Limitations"},{"location":"storage-backends/neo4j/","text":"Neo4j Neo4j is a native graph database, built from the ground up to leverage not only data but also data relationships. Neo4j connects data as it\u2019s stored, enabling queries never before imagined, at speeds never thought possible. \u2014 Neo4j Homepage Neo4j is a schema-less graph database written in Java with a simple query language called Cypher which is queried either via a transactional HTTP endpoint or through the Bolt protocol. Plume queries Neo4j via Cypher over the Bolt protocol. Driver Configuration and Usage Neo4j's driver can be created as follows: val driver = new Neo4jDriver ( hostname = \"localhost\" , port = 7474 , username = \"neo4j\" , password = \"neo4j\" ) One can optionally load indices with a call to: driver . buildSchema () The driver makes use of the official Neo4j Java driver . As described by the Gremlin driver's README, the following are the pros and cons of using the Gremlin driver over the Cypher one: Ideal Use Case Neo4j is a mature graph database vendor with a simple learning curve. The UI is mature and Cypher is a simple query language to get started with basic traversals. Neo4j provides a cloud service called Neo4j Aura which provides consumption-based pricing and much easier maintainability. The community for Neo4j is extremely large and so is the number of available resources and books. When combined with Plume, this schema-free graph database is ideal for rapid prototyping and quick deployments as the software is lightweight and UI very simple to use. With this does come limitations as query results cannot be visualized in more than a force-directed structure and, since it runs on the JVM, heapspace consumption can become a problem on extremely large code property graphs. Benefits The following benefits are obtained from the Neptune homepage: Supports clustering with master-slave topology. High read and write performance while maintaining data integrity with concurrent transaction support. Scalable architecture optimized for response times and ACID (Atomicity, Consistency, Isolation, Durability) compliant. Supports transactions and locking. Drivers and APIs for all major languages. Flexible data model to allow a change of schema on the fly. Provides high availability and real-time data analysis. Limitations Data is in-memory on each machine and disk seeks are expensive. So once system memory is hit, the performance slows down dramatically as data overflows on the disk. Master-slave architecture means that all writes are sent to master - but once a master is down, a new master election does not take long. This results in a write-bottleneck. Requires a lot of JVM configuration to use effectively, since queries and context are stored in the JVM heap, over-allocation to heap memory may result in increasing GC pauses and under-allocation may result in out-of-memory exceptions.","title":"Neo4j"},{"location":"storage-backends/neo4j/#neo4j","text":"Neo4j is a native graph database, built from the ground up to leverage not only data but also data relationships. Neo4j connects data as it\u2019s stored, enabling queries never before imagined, at speeds never thought possible. \u2014 Neo4j Homepage Neo4j is a schema-less graph database written in Java with a simple query language called Cypher which is queried either via a transactional HTTP endpoint or through the Bolt protocol. Plume queries Neo4j via Cypher over the Bolt protocol.","title":"Neo4j"},{"location":"storage-backends/neo4j/#driver-configuration-and-usage","text":"Neo4j's driver can be created as follows: val driver = new Neo4jDriver ( hostname = \"localhost\" , port = 7474 , username = \"neo4j\" , password = \"neo4j\" ) One can optionally load indices with a call to: driver . buildSchema () The driver makes use of the official Neo4j Java driver . As described by the Gremlin driver's README, the following are the pros and cons of using the Gremlin driver over the Cypher one:","title":"Driver Configuration and Usage"},{"location":"storage-backends/neo4j/#ideal-use-case","text":"Neo4j is a mature graph database vendor with a simple learning curve. The UI is mature and Cypher is a simple query language to get started with basic traversals. Neo4j provides a cloud service called Neo4j Aura which provides consumption-based pricing and much easier maintainability. The community for Neo4j is extremely large and so is the number of available resources and books. When combined with Plume, this schema-free graph database is ideal for rapid prototyping and quick deployments as the software is lightweight and UI very simple to use. With this does come limitations as query results cannot be visualized in more than a force-directed structure and, since it runs on the JVM, heapspace consumption can become a problem on extremely large code property graphs.","title":"Ideal Use Case"},{"location":"storage-backends/neo4j/#benefits","text":"The following benefits are obtained from the Neptune homepage: Supports clustering with master-slave topology. High read and write performance while maintaining data integrity with concurrent transaction support. Scalable architecture optimized for response times and ACID (Atomicity, Consistency, Isolation, Durability) compliant. Supports transactions and locking. Drivers and APIs for all major languages. Flexible data model to allow a change of schema on the fly. Provides high availability and real-time data analysis.","title":"Benefits"},{"location":"storage-backends/neo4j/#limitations","text":"Data is in-memory on each machine and disk seeks are expensive. So once system memory is hit, the performance slows down dramatically as data overflows on the disk. Master-slave architecture means that all writes are sent to master - but once a master is down, a new master election does not take long. This results in a write-bottleneck. Requires a lot of JVM configuration to use effectively, since queries and context are stored in the JVM heap, over-allocation to heap memory may result in increasing GC pauses and under-allocation may result in out-of-memory exceptions.","title":"Limitations"},{"location":"storage-backends/neptune/","text":"Amazon Neptune Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. \u2014 Amazon Neptune Homepage Amazon Neptune supports open graph APIs for both Gremlin and SPARQL where Plume communicates with Neptune using the former. Neptune does not expose it's API publically and must be queried via an EC2 instance in the same VPC as the Neptune cluster. More on this can be found on the documentation . Driver Configuration and Usage Neptune's driver can be created as follows: val driver = new NeptuneDriver ( hostname = \"<neptune-cluster-address-without-https>\" , port = 8182 , keyCertChainFile = \"<pem-certificate-path-here>\" ) The driver provides a wrapper over the Cluster builder and the process can be found in the documentation . One may only communicate via Neptune using HTTPS and thus the enableSsl is automatically set to true where the default port is 8182 if not specified. When using Neptune outside of production the keyCertChainFile need not be specified. Neptune has a bulk delete feature which is used when clearGraph is called and there are more than 100,000 nodes in the graph. This will take a minute or two but is much faster than the normal g.V().drop().iterate() especially when you find yourself with a graph of over a million vertices. Ideal Use Case Amazon Neptune forms part of the larger AWS cloud infrastructure as the primary graph database service. If one's services are run on AWS then Neptune would be a seamless fit within the existing ecosystem. Amazon Neptune supports popular graph models Property Graph and W3C's RDF, and their respective query languages Apache TinkerPop Gremlin and SPARQL useful for if both models are required by a single service. Neptune is designed to be highly available, durable, and fault tolerant for systems that aren't allowed to fail. Benefits The following benefits are obtained from the Neptune homepage: Supports open graph APIs Gremlin and SPARQL. High performance and scalability by supporting up to 15 low latency read replicas across three Availability Zones to scale read capacity and execute more than one-hundred thousand graph queries per second. Highly available, durable, and ACID (Atomicity, Consistency, Isolation, Durability) compliant. Neptune is designed to provide greater than 99.99% availability. It features fault-tolerant and self-healing storage built for the cloud that replicates six copies of your data across three Availability Zones. Neptune continuously backs up your data to Amazon S3, and transparently recovers from physical storage failures. Multiple levels of security for your database, including network isolation using Amazon VPC, support for IAM authentication for endpoint access, HTTPS encrypted client connections, encryption at rest using keys you create and control through AWS Key Management Service (KMS). Database management tasks such as hardware provisioning, software patching, setup, configuration, or backups are fully managed. Limitations The following notable limitations are extracted from this page : There is a size limit of 55 MB on the size of an individual property or label. In RDF terms, this means that the value in any column (S, P, O or G) of an RDF quad cannot exceed 55 MB. If you need to associate a larger object such as an image with a vertex or node in your graph, you can store it as a file in Amazon S3 and use the Amazon S3 path as the property or label. Most regions require an SSL connection. Each AWS account has limits for each Region on the number of Amazon Neptune and Amazon RDS resources that you can create. These resources include DB instances and DB clusters. After you reach a limit for a resource, additional calls to create that resource fail with an exception. Amazon Neptune is a virtual private cloud (VPC)\u2013only service. Additionally, instances do not allow access from outside the VPC.","title":"Amazon Neptune"},{"location":"storage-backends/neptune/#amazon-neptune","text":"Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. The core of Amazon Neptune is a purpose-built, high-performance graph database engine optimized for storing billions of relationships and querying the graph with milliseconds latency. \u2014 Amazon Neptune Homepage Amazon Neptune supports open graph APIs for both Gremlin and SPARQL where Plume communicates with Neptune using the former. Neptune does not expose it's API publically and must be queried via an EC2 instance in the same VPC as the Neptune cluster. More on this can be found on the documentation .","title":"Amazon Neptune"},{"location":"storage-backends/neptune/#driver-configuration-and-usage","text":"Neptune's driver can be created as follows: val driver = new NeptuneDriver ( hostname = \"<neptune-cluster-address-without-https>\" , port = 8182 , keyCertChainFile = \"<pem-certificate-path-here>\" ) The driver provides a wrapper over the Cluster builder and the process can be found in the documentation . One may only communicate via Neptune using HTTPS and thus the enableSsl is automatically set to true where the default port is 8182 if not specified. When using Neptune outside of production the keyCertChainFile need not be specified. Neptune has a bulk delete feature which is used when clearGraph is called and there are more than 100,000 nodes in the graph. This will take a minute or two but is much faster than the normal g.V().drop().iterate() especially when you find yourself with a graph of over a million vertices.","title":"Driver Configuration and Usage"},{"location":"storage-backends/neptune/#ideal-use-case","text":"Amazon Neptune forms part of the larger AWS cloud infrastructure as the primary graph database service. If one's services are run on AWS then Neptune would be a seamless fit within the existing ecosystem. Amazon Neptune supports popular graph models Property Graph and W3C's RDF, and their respective query languages Apache TinkerPop Gremlin and SPARQL useful for if both models are required by a single service. Neptune is designed to be highly available, durable, and fault tolerant for systems that aren't allowed to fail.","title":"Ideal Use Case"},{"location":"storage-backends/neptune/#benefits","text":"The following benefits are obtained from the Neptune homepage: Supports open graph APIs Gremlin and SPARQL. High performance and scalability by supporting up to 15 low latency read replicas across three Availability Zones to scale read capacity and execute more than one-hundred thousand graph queries per second. Highly available, durable, and ACID (Atomicity, Consistency, Isolation, Durability) compliant. Neptune is designed to provide greater than 99.99% availability. It features fault-tolerant and self-healing storage built for the cloud that replicates six copies of your data across three Availability Zones. Neptune continuously backs up your data to Amazon S3, and transparently recovers from physical storage failures. Multiple levels of security for your database, including network isolation using Amazon VPC, support for IAM authentication for endpoint access, HTTPS encrypted client connections, encryption at rest using keys you create and control through AWS Key Management Service (KMS). Database management tasks such as hardware provisioning, software patching, setup, configuration, or backups are fully managed.","title":"Benefits"},{"location":"storage-backends/neptune/#limitations","text":"The following notable limitations are extracted from this page : There is a size limit of 55 MB on the size of an individual property or label. In RDF terms, this means that the value in any column (S, P, O or G) of an RDF quad cannot exceed 55 MB. If you need to associate a larger object such as an image with a vertex or node in your graph, you can store it as a file in Amazon S3 and use the Amazon S3 path as the property or label. Most regions require an SSL connection. Each AWS account has limits for each Region on the number of Amazon Neptune and Amazon RDS resources that you can create. These resources include DB instances and DB clusters. After you reach a limit for a resource, additional calls to create that resource fail with an exception. Amazon Neptune is a virtual private cloud (VPC)\u2013only service. Additionally, instances do not allow access from outside the VPC.","title":"Limitations"},{"location":"storage-backends/overflowdb/","text":"OverflowDB ShiftLeft's OverflowDB is an in-memory graph database, which implements a swapping mechanism to deal with large graphs. \u2014 ShiftLeft GitHub Driver Configuration and Usage OverflowDb is an in-memory storage backend option with an overflow to disk mechanism when memory starts filling up. This can be obtained as follows: val driver = new OverflowDbDriver ( // To specifies if OverflowDb should write to disk when memory is constrained storageLocation = Some ( dbOutputFile ), // Percentage of the heap from when overflowing should begin to occur heapPercentageThreshold = 80 , // If specified, OverflowDB will measure and report serialization/deserialization timing averages serializationStatsEnabled = false , // Max call depth when tracking data-flows on calls to nodesReachableBy mallCallDepth = 2 ) This exported graph can then be loaded back into the driver by specifying the same path under storageLocation . Benefits of using OverflowDB Low memory footprint OverflowDB enforces a strict schema and uses implicit/virtual edges to help lower the memory footprint. When memory does become limited, OverflowDB will start serializing instances to disk and out of the heap to avoid OutOfMemoryError . By making use of a strict schema using domain classes, memory isn't wasted on many Map instances as is the case with TinkerGraph . High performance Due to the whole graph being in-memory the performance is fast (until swapping to disk occurs). Production There are many use cases where a dedicated storage backend is not necessary. Here are a few applications: For analysis on smaller programs or parts of programs where its feasible to store snapshots of graphs in something compact such as the bin format. To be part of a DevSecOps lifecycle where the process is run on low resource containers and the graph itself may be disposable but the results can be stored elsewhere. High performance is required, but no expertise/resources available for dedicated backends.","title":"OverflowDB"},{"location":"storage-backends/overflowdb/#overflowdb","text":"ShiftLeft's OverflowDB is an in-memory graph database, which implements a swapping mechanism to deal with large graphs. \u2014 ShiftLeft GitHub","title":"OverflowDB"},{"location":"storage-backends/overflowdb/#driver-configuration-and-usage","text":"OverflowDb is an in-memory storage backend option with an overflow to disk mechanism when memory starts filling up. This can be obtained as follows: val driver = new OverflowDbDriver ( // To specifies if OverflowDb should write to disk when memory is constrained storageLocation = Some ( dbOutputFile ), // Percentage of the heap from when overflowing should begin to occur heapPercentageThreshold = 80 , // If specified, OverflowDB will measure and report serialization/deserialization timing averages serializationStatsEnabled = false , // Max call depth when tracking data-flows on calls to nodesReachableBy mallCallDepth = 2 ) This exported graph can then be loaded back into the driver by specifying the same path under storageLocation .","title":"Driver Configuration and Usage"},{"location":"storage-backends/overflowdb/#benefits-of-using-overflowdb","text":"","title":"Benefits of using OverflowDB"},{"location":"storage-backends/overflowdb/#low-memory-footprint","text":"OverflowDB enforces a strict schema and uses implicit/virtual edges to help lower the memory footprint. When memory does become limited, OverflowDB will start serializing instances to disk and out of the heap to avoid OutOfMemoryError . By making use of a strict schema using domain classes, memory isn't wasted on many Map instances as is the case with TinkerGraph .","title":"Low memory footprint"},{"location":"storage-backends/overflowdb/#high-performance","text":"Due to the whole graph being in-memory the performance is fast (until swapping to disk occurs).","title":"High performance"},{"location":"storage-backends/overflowdb/#production","text":"There are many use cases where a dedicated storage backend is not necessary. Here are a few applications: For analysis on smaller programs or parts of programs where its feasible to store snapshots of graphs in something compact such as the bin format. To be part of a DevSecOps lifecycle where the process is run on low resource containers and the graph itself may be disposable but the results can be stored elsewhere. High performance is required, but no expertise/resources available for dedicated backends.","title":"Production"},{"location":"storage-backends/tigergraph/","text":"TigerGraph TigerGraph is the world\u2019s fastest graph analytics platform designed to unleash the power of interconnected data for deeper insights and better outcomes. TigerGraph fulfills the true promise and benefits of the graph platform by tackling the toughest data challenges in real time, no matter how large or complex the dataset. TigerGraph supports applications such as IoT, AI and machine learning to make sense of ever-changing big data. \u2014 TigerGraph About TigerGraph is a native parallel graph with a C++ codebase. Plume communicates with TigerGraph via a REST interface which can be secured with HTTPS and an authentication token. Since TigerGraph uses a schema there is additional setup beforehand but this is made simple by making use of Docker. Driver Configuration and Usage Since TigerGraph makes use of REST, it only requires server information and (optionally) an authentication e.g. when using TigerGraph Cloud . By default the hostname will be 127.0.0.1, port 9000 and protocol HTTP. The driver is created as follows: val driver = new TigerGraphDriver ( hostname = \"localhost\" , restPpPort = 9000 , gsqlPort = 14240 , username = \"tigergraph\" , password = \"tigergraph\" , ) Plume will make use of a graph called \"cpg\". This schema can be built by calling: driver . buildSchema () This wraps around the GsqlCli JAR. This JAR makes a call to System::exit which is temporarily overriden until the calls to GsqlCli are done so you may notice some exceptions thrown on stdout . Ideal Use Case TigerGraph is fast and efficient both in terms of storage and memory consumption. TigerGraph provides a developer and enterprise edition but one can easily get started with a free TigerGraph Cloud instance if resources are limited. Like TinkerGraph, TigerGraph is ideal for graph exploration as it comes with a built-in graph visualizer using GraphStudio. Benefits Free to use for development and the TigerGraph Cloud free tier instance comes with a generous resource allocation. Distributed capabilities and ACID compliant. Fast execution of parallel graph algorithms. Real-time capability for streaming updates and inserts using REST. Ability to unify real-time analytics with large-scale offline data processing. Ability to traverse hundreds of millions of vertices/edges per second per machine. Ability to load 50 to 150 GB of data per hour, per machine. Ability to stream 2B+ daily events in real-time. High compression rates for low resource consumption. Limitations Enforced schema requires compulsory setup to use with Plume.","title":"TigerGraph"},{"location":"storage-backends/tigergraph/#tigergraph","text":"TigerGraph is the world\u2019s fastest graph analytics platform designed to unleash the power of interconnected data for deeper insights and better outcomes. TigerGraph fulfills the true promise and benefits of the graph platform by tackling the toughest data challenges in real time, no matter how large or complex the dataset. TigerGraph supports applications such as IoT, AI and machine learning to make sense of ever-changing big data. \u2014 TigerGraph About TigerGraph is a native parallel graph with a C++ codebase. Plume communicates with TigerGraph via a REST interface which can be secured with HTTPS and an authentication token. Since TigerGraph uses a schema there is additional setup beforehand but this is made simple by making use of Docker.","title":"TigerGraph"},{"location":"storage-backends/tigergraph/#driver-configuration-and-usage","text":"Since TigerGraph makes use of REST, it only requires server information and (optionally) an authentication e.g. when using TigerGraph Cloud . By default the hostname will be 127.0.0.1, port 9000 and protocol HTTP. The driver is created as follows: val driver = new TigerGraphDriver ( hostname = \"localhost\" , restPpPort = 9000 , gsqlPort = 14240 , username = \"tigergraph\" , password = \"tigergraph\" , ) Plume will make use of a graph called \"cpg\". This schema can be built by calling: driver . buildSchema () This wraps around the GsqlCli JAR. This JAR makes a call to System::exit which is temporarily overriden until the calls to GsqlCli are done so you may notice some exceptions thrown on stdout .","title":"Driver Configuration and Usage"},{"location":"storage-backends/tigergraph/#ideal-use-case","text":"TigerGraph is fast and efficient both in terms of storage and memory consumption. TigerGraph provides a developer and enterprise edition but one can easily get started with a free TigerGraph Cloud instance if resources are limited. Like TinkerGraph, TigerGraph is ideal for graph exploration as it comes with a built-in graph visualizer using GraphStudio.","title":"Ideal Use Case"},{"location":"storage-backends/tigergraph/#benefits","text":"Free to use for development and the TigerGraph Cloud free tier instance comes with a generous resource allocation. Distributed capabilities and ACID compliant. Fast execution of parallel graph algorithms. Real-time capability for streaming updates and inserts using REST. Ability to unify real-time analytics with large-scale offline data processing. Ability to traverse hundreds of millions of vertices/edges per second per machine. Ability to load 50 to 150 GB of data per hour, per machine. Ability to stream 2B+ daily events in real-time. High compression rates for low resource consumption.","title":"Benefits"},{"location":"storage-backends/tigergraph/#limitations","text":"Enforced schema requires compulsory setup to use with Plume.","title":"Limitations"},{"location":"storage-backends/tinkergraph/","text":"TinkerGraph TinkerGraph is a lightweight, POJO based, in-memory property graph that serves as the reference implementation for the property graph model . If you have a small graph that can be loaded and saved using the GraphML reader and writer library, then TinkerGraph can be handy to use. It is also great for use in writing unit tests in place of other implementations that require greater resources. \u2014 TinkerGraph GitHub Driver Configuration and Usage TinkerGraph is an in-memory storage backend option which can be obtained as follows: val driver = new TinkerGraphDriver () On construction the following configuration is automatically added to the BaseConfiguration object: config . setProperty ( \"gremlin.graph\" , \"org.apache.tinkerpop.gremlin.tinkergraph.structure.TinkerGraph\" ) No additional configuration is typically required as the graph is held in memory in the heap space allocated to the Java Virtual Machine running the application making use of the Plume library. The data held in this graph will be deleted upon the termination of the application or process using Plume but this graph can be imported and exported to XML, JSON, or Kryo using the importGraph(filePath: String) and exportGraph(filePath: String) methods respectively. Ideal Use Case Rapid testing The in-memory graph database option is used primarily for testing and graph exploration. TinkerGraph can be paired with a graph visualizer such as Cytoscape as it recognized GraphML. This is the recommended backend to get familiar with Plume and how it works as it is also the simplest to configure. Production There are many use cases where a dedicated storage backend is not necessary. Here are a few applications: For analysis on smaller programs or parts of programs where its feasible to store snapshots of graphs in something compact such as Kryo. To be part of a DevSecOps lifecycle where the process is run on low resource containers and the graph itself may be disposable but the results can be stored elsewhere. High performance is required, but no expertise/resources available for dedicated backends. Limitations Does not scale well for large programs as uncompressed POJOs represent the graph and everything is stored in the heap of a single JVM. No transactions and no resilience to failures. Once a transaction has started, although chance of failure is low, there is no clean rollback if a failure occurs in the middle of a transaction.","title":"TinkerGraph"},{"location":"storage-backends/tinkergraph/#tinkergraph","text":"TinkerGraph is a lightweight, POJO based, in-memory property graph that serves as the reference implementation for the property graph model . If you have a small graph that can be loaded and saved using the GraphML reader and writer library, then TinkerGraph can be handy to use. It is also great for use in writing unit tests in place of other implementations that require greater resources. \u2014 TinkerGraph GitHub","title":"TinkerGraph"},{"location":"storage-backends/tinkergraph/#driver-configuration-and-usage","text":"TinkerGraph is an in-memory storage backend option which can be obtained as follows: val driver = new TinkerGraphDriver () On construction the following configuration is automatically added to the BaseConfiguration object: config . setProperty ( \"gremlin.graph\" , \"org.apache.tinkerpop.gremlin.tinkergraph.structure.TinkerGraph\" ) No additional configuration is typically required as the graph is held in memory in the heap space allocated to the Java Virtual Machine running the application making use of the Plume library. The data held in this graph will be deleted upon the termination of the application or process using Plume but this graph can be imported and exported to XML, JSON, or Kryo using the importGraph(filePath: String) and exportGraph(filePath: String) methods respectively.","title":"Driver Configuration and Usage"},{"location":"storage-backends/tinkergraph/#ideal-use-case","text":"","title":"Ideal Use Case"},{"location":"storage-backends/tinkergraph/#rapid-testing","text":"The in-memory graph database option is used primarily for testing and graph exploration. TinkerGraph can be paired with a graph visualizer such as Cytoscape as it recognized GraphML. This is the recommended backend to get familiar with Plume and how it works as it is also the simplest to configure.","title":"Rapid testing"},{"location":"storage-backends/tinkergraph/#production","text":"There are many use cases where a dedicated storage backend is not necessary. Here are a few applications: For analysis on smaller programs or parts of programs where its feasible to store snapshots of graphs in something compact such as Kryo. To be part of a DevSecOps lifecycle where the process is run on low resource containers and the graph itself may be disposable but the results can be stored elsewhere. High performance is required, but no expertise/resources available for dedicated backends.","title":"Production"},{"location":"storage-backends/tinkergraph/#limitations","text":"Does not scale well for large programs as uncompressed POJOs represent the graph and everything is stored in the heap of a single JVM. No transactions and no resilience to failures. Once a transaction has started, although chance of failure is low, there is no clean rollback if a failure occurs in the middle of a transaction.","title":"Limitations"}]}